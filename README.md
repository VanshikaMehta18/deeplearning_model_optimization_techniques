This project explores different techniques for optimizing deep learning models so they can run efficiently on devices with limited computational resources, such as smartphones, smartwatches, and edge devices.

We focus on three core optimization techniques:

1. Quantization
2. Pruning
3. Distillation
   
Each technique is implemented using PyTorch.

